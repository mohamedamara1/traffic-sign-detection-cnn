{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77fdff7f-a737-40a0-8b3f-6078b6126b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/user/miniconda3/envs/tf/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n",
      "Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4d90c6-4d80-4b38-b9c4-366793f2355e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 11:51:41.071488: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-08 11:51:41.084163: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-08 11:51:41.087829: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-08 11:51:41.097267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D, MaxPool2D, Dense, Flatten, Dropout\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseCategoricalCrossentropy\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import random\n",
    "from keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281fdb6-c39a-451c-b959-777716bacfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def fetch_image_with_detection_and_visualization(data_folder):\n",
    "    \"\"\"\n",
    "    Fetch images and corresponding labels, perform detection (crop based on bounding boxes),\n",
    "    and resize to fit the classifier's input. Also visualizes a sample cropped image.\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    valid_data = []\n",
    "    cropped_samples = []  # To store a few sample cropped images for visualization\n",
    "    \n",
    "    splits = {\n",
    "        \"train\": os.path.join(data_folder, \"train\"),\n",
    "        \"test\": os.path.join(data_folder, \"test\"),\n",
    "        \"valid\": os.path.join(data_folder, \"valid\")\n",
    "    }\n",
    "    \n",
    "    for split_name, split_path in splits.items():\n",
    "        images_folder = os.path.join(split_path, \"images\")\n",
    "        labels_folder = os.path.join(split_path, \"labels\")\n",
    "        \n",
    "        for image_name in os.listdir(images_folder):\n",
    "            label_name = os.path.splitext(image_name)[0] + \".txt\"\n",
    "            label_path = os.path.join(labels_folder, label_name)\n",
    "            image_path = os.path.join(images_folder, image_name)\n",
    "            \n",
    "            if not os.path.exists(label_path):\n",
    "                print(f\"Warning: Label file not found for {image_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Load the image without converting to grayscale\n",
    "            img = Image.open(image_path)\n",
    "            image_width, image_height = img.size\n",
    "            img = np.array(img)\n",
    "\n",
    "            # Parse the label file for bounding boxes\n",
    "            with open(label_path, \"r\") as label_file:\n",
    "                lines = label_file.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    class_id = int(parts[0])  # First value is the class ID\n",
    "                    \n",
    "                    # Parse normalized bounding box values\n",
    "                    x_center, y_center, width, height = map(float, parts[1:])\n",
    "                    \n",
    "                    # Convert to pixel coordinates\n",
    "                    x_min = int((x_center - width / 2) * image_width)\n",
    "                    y_min = int((y_center - height / 2) * image_height)\n",
    "                    x_max = int((x_center + width / 2) * image_width)\n",
    "                    y_max = int((y_center + height / 2) * image_height)\n",
    "\n",
    "                    # Crop the detected region\n",
    "                    cropped_img = img[y_min:y_max, x_min:x_max]\n",
    "                    \n",
    "                    # Resize to match classifier input\n",
    "                    resized_img = cv2.resize(cropped_img, (90, 90), interpolation=cv2.INTER_AREA)\n",
    "                    \n",
    "                    # Normalize the image\n",
    "                    resized_img = resized_img / 255.0\n",
    "                    \n",
    "                    # Store a sample cropped image for visualization (only store the first few samples)\n",
    "                    if len(cropped_samples) < 5:  # Limit to 5 samples\n",
    "                        cropped_samples.append(resized_img)\n",
    "                    \n",
    "                    # Assign the cropped image and its label to the appropriate split\n",
    "                    if split_name == \"train\":\n",
    "                        train_data.append((resized_img, class_id))\n",
    "                    elif split_name == \"test\":\n",
    "                        test_data.append((resized_img, class_id))\n",
    "                    elif split_name == \"valid\":\n",
    "                        valid_data.append((resized_img, class_id))\n",
    "    \n",
    "    # Visualize a few cropped images\n",
    "    if cropped_samples:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i, cropped_img in enumerate(cropped_samples):\n",
    "            plt.subplot(1, len(cropped_samples), i+1)\n",
    "            plt.imshow(cropped_img)\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return train_data, test_data, valid_data\n",
    "\n",
    "\n",
    "\n",
    "# Fetch and visualize the cropped images\n",
    "data_folder = \"./DataSet\"\n",
    "train_data, test_data, valid_data = fetch_image_with_detection_and_visualization(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeea084-4600-4072-bb09-e2462389f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data features and labels\n",
    "training_data_features, training_data_labels = zip(*train_data)\n",
    "training_data_features = np.array(training_data_features).reshape(-1, 90, 90, 3)  # Add channel dimension\n",
    "training_data_labels = np.array(training_data_labels)\n",
    "\n",
    "# Prepare validation data\n",
    "validation_data_features, validation_data_labels = zip(*valid_data)\n",
    "validation_data_features = np.array(validation_data_features).reshape(-1, 90, 90, 3)\n",
    "validation_data_labels = np.array(validation_data_labels)\n",
    "\n",
    "# Prepare test data\n",
    "test_data_features, test_data_labels = zip(*test_data)\n",
    "test_data_features = np.array(test_data_features).reshape(-1, 90, 90, 3)\n",
    "test_data_labels = np.array(test_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14346d60-63d5-42c8-bc60-69bb5b767d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of features and labels\n",
    "print(f\"Training data features shape: {np.array(training_data_features).shape}\")\n",
    "print(f\"Training data labels shape: {np.array(training_data_labels).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04437d5a-f459-448e-8767-0613c75021cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, label_counts = np.unique(training_data_labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, label_counts):\n",
    "    print(f\"Label {label}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43886b-8e1b-4d3d-9183-71455028a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel = Sequential()\n",
    "cnnModel.add(Conv2D(16,(3,3), padding=\"same\", input_shape=(90, 90, 3), activation='relu'))\n",
    "print(cnnModel(training_data_features).shape)\n",
    "cnnModel.add(MaxPool2D((2,2), strides=None, padding=\"same\"))\n",
    "cnnModel.add(Conv2D(32,(3,3), padding=\"same\", activation='relu'))\n",
    "cnnModel.add(MaxPool2D((2,2), strides=None, padding=\"same\"))\n",
    "cnnModel.add(Conv2D(64,(5,5), padding=\"same\", activation='relu'))\n",
    "cnnModel.add(MaxPool2D((2,2), strides=None, padding=\"same\"))\n",
    "cnnModel.add(Conv2D(128,(7,7), padding=\"same\", activation='relu'))\n",
    "cnnModel.add(MaxPool2D((2,2), strides=None, padding=\"same\"))\n",
    "cnnModel.add(Flatten())\n",
    "cnnModel.add(Dense(232, activation='relu'))\n",
    "cnnModel.add(Dense(116, activation='relu'))\n",
    "cnnModel.add(Dense(15, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0559d0-267f-45cf-9d92-c3c93561d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7bcae-2b3d-4431-8d51-c3fe69e92be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b1a49-95f3-41d5-8259-ef3ed50bfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "SignTraffic = cnnModel.fit(\n",
    "    training_data_features, \n",
    "    training_data_labels, \n",
    "    validation_data=(validation_data_features, validation_data_labels),\n",
    "    epochs=10, \n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c77e3e-d055-4aa2-a0ff-a6de75febf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(SignTraffic.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd765d-efa7-469f-9f33-522860e4536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Assuming history is the result of the model's fit() method\n",
    "plt.plot(SignTraffic.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(SignTraffic.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(SignTraffic.history['loss'], label='Training Loss')\n",
    "plt.plot(SignTraffic.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931b6d1-e350-4f66-9033-d12f74f43030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Predict the labels on the test data\n",
    "test_predictions = cnnModel.predict(test_data_features)\n",
    "# Convert predictions from probabilities to class labels (use argmax to get the index of the highest probability)\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(test_data_labels, test_predictions)\n",
    "\n",
    "# Plot the confusion matrix using seaborn for better visualization\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(test_data_labels), yticklabels=np.unique(test_data_labels))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b853e3-6651-4ba8-b834-f8455ca1c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Calculate metrics for each class\n",
    "for i in range(cm.shape[0]):\n",
    "    tp = cm[i, i]\n",
    "    fp = cm[:, i].sum() - tp  # sum of the column - TP\n",
    "    fn = cm[i, :].sum() - tp  # sum of the row - TP\n",
    "    tn = cm.sum() - (tp + fp + fn)\n",
    "    \n",
    "    # Calculate Precision, Recall, F1-Score, and Accuracy\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"True Positive (TP): {tp}\")\n",
    "    print(f\"False Positive (FP): {fp}\")\n",
    "    print(f\"False Negative (FN): {fn}\")\n",
    "    print(f\"True Negative (TN): {tn}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9651b6c5-9fb4-42cb-bbd1-51a95c4c791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_data_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f80f54-a069-44ae-a972-ac30fa5dc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da1565-6fec-41bd-b41a-c3f30d44012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14147c-4fc3-48a3-bff0-bb8e569267d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    print(f\"Original Image Size: {img.size}\")\n",
    "    img = img.resize((90, 90))\n",
    "    img_array = np.array(img)\n",
    "    # If the image is grayscale (2D array), convert it to 3 channels (RGB)\n",
    "    if len(img_array.shape) == 2:  # Grayscale image (2D array)\n",
    "        img_array = np.stack([img_array] * 3, axis=-1)\n",
    "    \n",
    "    img_array = img_array / 255.0\n",
    "    img_array = img_array.reshape(-1, 90, 90, 3)\n",
    "    print(f\"Image Resize: {img_array.shape}\")\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa4e6ac-fa92-4f76-bae6-c70446768c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model):\n",
    "    img_array = preprocess_image(image_path)   \n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "    predicted_probability = np.max(predictions, axis=1)[0]\n",
    "    return predicted_class, predicted_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81da16e-9bff-4873-9db6-2628f47a6872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "test_path = r\"test_img\\70_2.jpeg\" \n",
    "expected_size=(90, 90)\n",
    "predicted_class, predicted_probability = predict_image(test_path, cnnModel)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Prediction Probability: {predicted_probability:.4f}\")\n",
    "# Display the image\n",
    "img = Image.open(test_path)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.title(f\"Class: {predicted_class}, Probability: {predicted_probability:.4f}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
